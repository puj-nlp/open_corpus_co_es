{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-07T22:50:01.261940Z",
     "start_time": "2025-07-07T22:49:22.301634Z"
    }
   },
   "source": "!pip install --force-reinstall git+https://github.com/puj-nlp/open_corpus_co_es.",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Collecting git+https://github.com/puj-nlp/open_corpus_co_es.git\n",
      "  Cloning https://github.com/puj-nlp/open_corpus_co_es.git to c:\\users\\phineas\\appdata\\local\\temp\\pip-req-build-q69vporf\n",
      "  Resolved https://github.com/puj-nlp/open_corpus_co_es.git to commit 323b6a2e814b112fb57913049a205f974eed38c2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting nltk (from open_corpus_co_es==0.1.0)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pandas (from open_corpus_co_es==0.1.0)\n",
      "  Downloading pandas-2.3.1-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting gdown (from open_corpus_co_es==0.1.0)\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pyarrow (from open_corpus_co_es==0.1.0)\n",
      "  Using cached pyarrow-20.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting fastparquet (from open_corpus_co_es==0.1.0)\n",
      "  Using cached fastparquet-2024.11.0-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting tqdm (from open_corpus_co_es==0.1.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting numpy (from fastparquet->open_corpus_co_es==0.1.0)\n",
      "  Using cached numpy-2.3.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting cramjam>=2.3 (from fastparquet->open_corpus_co_es==0.1.0)\n",
      "  Using cached cramjam-2.10.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting fsspec (from fastparquet->open_corpus_co_es==0.1.0)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting packaging (from fastparquet->open_corpus_co_es==0.1.0)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->open_corpus_co_es==0.1.0)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->open_corpus_co_es==0.1.0)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->open_corpus_co_es==0.1.0)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting beautifulsoup4 (from gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting requests[socks] (from gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting click (from nltk->open_corpus_co_es==0.1.0)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk->open_corpus_co_es==0.1.0)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk->open_corpus_co_es==0.1.0)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting colorama (from tqdm->open_corpus_co_es==0.1.0)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->open_corpus_co_es==0.1.0)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests[socks]->gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests[socks]->gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests[socks]->gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests[socks]->gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown->open_corpus_co_es==0.1.0)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached fastparquet-2024.11.0-cp312-cp312-win_amd64.whl (673 kB)\n",
      "Downloading pandas-2.3.1-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.8/11.0 MB 35.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 27.4 MB/s eta 0:00:00\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached pyarrow-20.0.0-cp312-cp312-win_amd64.whl (25.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached cramjam-2.10.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Using cached numpy-2.3.1-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl (105 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: open_corpus_co_es\n",
      "  Building wheel for open_corpus_co_es (pyproject.toml): started\n",
      "  Building wheel for open_corpus_co_es (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for open_corpus_co_es: filename=open_corpus_co_es-0.1.0-py3-none-any.whl size=12305 sha256=8530b881040842adc36de4f6e907176945c88f9903bcf5e4a3d3c5ff9cd356a8\n",
      "  Stored in directory: C:\\Users\\phineas\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-qq39qer_\\wheels\\ba\\a9\\ce\\3c170a30de35055d7c3e763c99659bc0ccdada339f9c1d350e\n",
      "Successfully built open_corpus_co_es\n",
      "Installing collected packages: pytz, urllib3, tzdata, typing-extensions, soupsieve, six, regex, PySocks, pyarrow, packaging, numpy, joblib, idna, fsspec, filelock, cramjam, colorama, charset_normalizer, certifi, tqdm, requests, python-dateutil, click, beautifulsoup4, pandas, nltk, gdown, fastparquet, open_corpus_co_es\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.4.0\n",
      "    Uninstalling urllib3-2.4.0:\n",
      "      Successfully uninstalled urllib3-2.4.0\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.14.0\n",
      "    Uninstalling typing_extensions-4.14.0:\n",
      "      Successfully uninstalled typing_extensions-4.14.0\n",
      "  Attempting uninstall: soupsieve\n",
      "    Found existing installation: soupsieve 2.7\n",
      "    Uninstalling soupsieve-2.7:\n",
      "      Successfully uninstalled soupsieve-2.7\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: PySocks\n",
      "    Found existing installation: PySocks 1.7.1\n",
      "    Uninstalling PySocks-1.7.1:\n",
      "      Successfully uninstalled PySocks-1.7.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 20.0.0\n",
      "    Uninstalling pyarrow-20.0.0:\n",
      "      Successfully uninstalled pyarrow-20.0.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.0\n",
      "    Uninstalling numpy-2.3.0:\n",
      "      Successfully uninstalled numpy-2.3.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.5.1\n",
      "    Uninstalling joblib-1.5.1:\n",
      "      Successfully uninstalled joblib-1.5.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.18.0\n",
      "    Uninstalling filelock-3.18.0:\n",
      "      Successfully uninstalled filelock-3.18.0\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.2\n",
      "    Uninstalling charset-normalizer-3.4.2:\n",
      "      Successfully uninstalled charset-normalizer-3.4.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.4.26\n",
      "    Uninstalling certifi-2025.4.26:\n",
      "      Successfully uninstalled certifi-2025.4.26\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.2.1\n",
      "    Uninstalling click-8.2.1:\n",
      "      Successfully uninstalled click-8.2.1\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.13.4\n",
      "    Uninstalling beautifulsoup4-4.13.4:\n",
      "      Successfully uninstalled beautifulsoup4-4.13.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.0\n",
      "    Uninstalling pandas-2.3.0:\n",
      "      Successfully uninstalled pandas-2.3.0\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.6\n",
      "    Uninstalling nltk-3.6:\n",
      "      Successfully uninstalled nltk-3.6\n",
      "  Attempting uninstall: gdown\n",
      "    Found existing installation: gdown 5.2.0\n",
      "    Uninstalling gdown-5.2.0:\n",
      "      Successfully uninstalled gdown-5.2.0\n",
      "  Attempting uninstall: open_corpus_co_es\n",
      "    Found existing installation: open_corpus_co_es 0.1.0\n",
      "    Not uninstalling open-corpus-co-es at c:\\project\\javeriana\\repos\\open_corpus_co_es, outside environment C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\n",
      "    Can't uninstall 'open_corpus_co_es'. No files were found to uninstall.\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.4 certifi-2025.6.15 charset_normalizer-3.4.2 click-8.2.1 colorama-0.4.6 cramjam-2.10.0 fastparquet-2024.11.0 filelock-3.18.0 fsspec-2025.5.1 gdown-5.2.0 idna-3.10 joblib-1.5.1 nltk-3.9.1 numpy-2.3.1 open_corpus_co_es-0.1.0 packaging-25.0 pandas-2.3.1 pyarrow-20.0.0 python-dateutil-2.9.0.post0 pytz-2025.2 regex-2024.11.6 requests-2.32.4 six-1.17.0 soupsieve-2.7 tqdm-4.67.1 typing-extensions-4.14.1 tzdata-2025.2 urllib3-2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/puj-nlp/open_corpus_co_es.git 'C:\\Users\\phineas\\AppData\\Local\\Temp\\pip-req-build-q69vporf'\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\~egex'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\~yarrow.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\~yarrow'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\~harset_normalizer'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T22:46:53.703066Z",
     "start_time": "2025-07-07T22:46:51.245027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from open_corpus_co_es.loader import load_corpus, list_corpus\n",
    "from open_corpus_co_es.downloader import download_corpus\n",
    "corpus_name = \"laudato_si\"\n",
    "download_corpus(corpus_name, force=True)\n",
    "datos = load_corpus(corpus_name)\n",
    "type(datos)"
   ],
   "id": "dbc2d9a7d0a3e00e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - ⬇️ Cargando corpus 'laudato_si'...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "[ERROR] No se encontraron archivos válidos en C:\\Users\\phineas/.open_corpus_co_es/data\\laudato_si\\laudato_si.zip. Verifica que el archivo ZIP esté bien estructurado.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mException\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mopen_corpus_co_es\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdownloader\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m download_corpus\n\u001B[32m      3\u001B[39m corpus_name = \u001B[33m\"\u001B[39m\u001B[33mlaudato_si\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43mdownload_corpus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m datos = load_corpus(corpus_name)\n\u001B[32m      6\u001B[39m \u001B[38;5;28mtype\u001B[39m(datos)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\open_corpus_co_es\\downloader.py:131\u001B[39m, in \u001B[36mdownload_corpus\u001B[39m\u001B[34m(name, force)\u001B[39m\n\u001B[32m    129\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m     \u001B[43mvalidate_corpus_structure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfinal_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    133\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m✅ Corpus \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m listo en: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\open_corpus_co_es\\downloader.py:60\u001B[39m, in \u001B[36mvalidate_corpus_structure\u001B[39m\u001B[34m(corpus_dir, expected_subfolder)\u001B[39m\n\u001B[32m     57\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m found_valid_file:\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m[ERROR] No se encontraron archivos válidos en \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_to_check\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     61\u001B[39m                     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mVerifica que el archivo ZIP esté bien estructurado.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     63\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m✅ Estructura de corpus o recurso válida en: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_to_check\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mException\u001B[39m: [ERROR] No se encontraron archivos válidos en C:\\Users\\phineas/.open_corpus_co_es/data\\laudato_si\\laudato_si.zip. Verifica que el archivo ZIP esté bien estructurado."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T14:47:50.802790Z",
     "start_time": "2025-06-30T14:47:50.797020Z"
    }
   },
   "cell_type": "code",
   "source": "datos.keys()",
   "id": "5358836ca81bf7c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['english_en_chapter_1', 'english_en_chapter_2', 'english_en_chapter_3', 'english_en_chapter_4', 'english_en_chapter_5', 'english_en_chapter_6', 'english_en_encyclical-laudato-si', 'espanol_desktop', 'espanol_es_capitulo_1', 'espanol_es_capitulo_2', 'espanol_es_capitulo_3', 'espanol_es_capitulo_4', 'espanol_es_capitulo_5', 'espanol_es_capitulo_6', 'espanol_es_enciclica-laudato-si'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T14:48:24.069688Z",
     "start_time": "2025-06-30T14:48:24.064362Z"
    }
   },
   "cell_type": "code",
   "source": "type(datos[\"english_en_chapter_1\"])",
   "id": "7691c4667133afd3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:16:31.365333Z",
     "start_time": "2025-06-30T15:16:31.359480Z"
    }
   },
   "cell_type": "code",
   "source": "datos[\"english_en_chapter_1\"]",
   "id": "46e6ffe0018f5fdb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:16:39.041399Z",
     "start_time": "2025-06-30T15:16:39.005927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from open_corpus_co_es.loader import load_corpus, list_corpus\n",
    "from open_corpus_co_es.downloader import download_corpus\n",
    "corpus_name = \"ods\"\n",
    "download_corpus(corpus_name)\n",
    "datos = load_corpus(corpus_name)\n",
    "type(datos)"
   ],
   "id": "fe48e082275f24d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - [INFO] Corpus 'ods' ya está cargado. Usa force=True para forzar descarga.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T15:18:22.304648Z",
     "start_time": "2025-06-30T15:18:22.299557Z"
    }
   },
   "cell_type": "code",
   "source": "datos[0]",
   "id": "674dd8632ddbd41c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\nObjetivo 1. Poner fin a la pobreza en todas sus formas y en todo el mundo \\n\\n\\n \\n\\n1.1 \\nDe aquí a 2030  erradicar para todas las personas y en todo el mundo la \\npobreza extrema(actualmente se considera que sufren pobreza extrema las personas \\nque viven con menos de 1 25 dólares de los Estados Unidos al día) \\n\\n\\n1.2 \\nDe aquí a 2030  reducir al menos a la mitad la proporción de hombres  mujeres \\ny niños de todas las edades que viven en la pobreza en todas sus dimensiones con \\narreglo a las definiciones nacionales \\n\\n\\n1.3 \\nImplementar a nivel nacional sistemas y medidas apropiados de protección \\nsocial para todos  incluidos niveles mínimos  y  de aquí a 2030  lograr una amplia \\ncobertura de las personas pobres y vulnerables \\n\\n\\n1.4 \\nDe aquí a 2030  garantizar que todos los hombres y mujeres  en particular los \\npobres y los vulnerables  tengan los mismos derechos a los recursos económicos y \\nacceso a los servicios básicos  la propiedad y el control de la tierra y otros bienes  la \\nherencia  los recursos naturales  las nuevas tecnologías apropiadas y los servicios \\nfinancieros  incluida la microfinanciación \\n\\n\\n1.5 \\nDe aquí a 2030  fomentar la resiliencia de los pobres y las personas que se \\nencuentran en situaciones de vulnerabilidad y reducir su exposición y vulnerabilidad \\na los fenómenos extremos relacionados con el clima y otras perturbaciones y \\ndesastres económicos  sociales y ambientales \\n\\n\\n1.a \\nGarantizar una movilización significativa de recursos procedentes de diversas \\nfuentes  incluso mediante la mejora de la cooperación para el desarrollo  a fin de \\nproporcionar medios suficientes y previsibles a los países en desarrollo  en \\nparticular los países menos adelantados  para que implementen programas y \\npolíticas encaminados a poner fin a la pobreza en todas sus dimensiones \\n\\n\\n1.b \\nCrear marcos normativos sólidos en los planos nacional  regional e \\ninternacional  sobre la base de estrategias de desarrollo en favor de los pobres que \\ntengan en cuenta las cuestiones de género  a fin de apoyar la inversión acelerada en \\nmedidas para erradicar la pobreza \\n',\n",
       " 'Objetivo': 'Objetivo 1',\n",
       " 'Unnamed: 2': ' '}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T23:56:27.819703Z",
     "start_time": "2025-06-23T23:56:26.279394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from open_corpus_co_es.loader import load_corpus, list_corpus\n",
    "from open_corpus_co_es.downloader import download_corpus\n",
    "corpus_name = \"ods\"\n",
    "download_corpus(corpus_name, force=True)\n",
    "datos = load_corpus(corpus_name)\n",
    "print(type(datos))\n",
    "datos"
   ],
   "id": "ce24ca6a7b9ebf0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reescribiendo corpus 'ods'...\n",
      "⬇️ Cargando corpus 'ods' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 288.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Corpus 'ods' listo en: C:\\Users\\phineas/.open_corpus_co_es/data\\ods\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Couldn't deserialize thrift: TProtocolException: Invalid data\nDeserializing page header failed.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      3\u001B[39m corpus_name = \u001B[33m\"\u001B[39m\u001B[33mods\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      4\u001B[39m download_corpus(corpus_name, force=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m datos = \u001B[43mload_corpus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(datos))\n\u001B[32m      7\u001B[39m datos\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\open_corpus_co_es\\loader.py:145\u001B[39m, in \u001B[36mload_corpus\u001B[39m\u001B[34m(name)\u001B[39m\n\u001B[32m    142\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m documentos\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m files:\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m     content = \u001B[43mextract_text_from_file\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    146\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(content, \u001B[38;5;28mlist\u001B[39m):\n\u001B[32m    147\u001B[39m         all_text.extend(content)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\open_corpus_co_es\\loader.py:80\u001B[39m, in \u001B[36mextract_text_from_file\u001B[39m\u001B[34m(path)\u001B[39m\n\u001B[32m     78\u001B[39m     df = pd.read_excel(path)\n\u001B[32m     79\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m ext == \u001B[33m\"\u001B[39m\u001B[33m.parquet\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m80\u001B[39m     df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     81\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m ext \u001B[38;5;129;01min\u001B[39;00m [\u001B[33m\"\u001B[39m\u001B[33m.jsonl\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m.json\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m     82\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, encoding=\u001B[33m\"\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001B[39m, in \u001B[36mread_parquet\u001B[39m\u001B[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001B[39m\n\u001B[32m    666\u001B[39m     use_nullable_dtypes = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    667\u001B[39m check_dtype_backend(dtype_backend)\n\u001B[32m--> \u001B[39m\u001B[32m669\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    670\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    671\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    672\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    673\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    674\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_nullable_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    675\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype_backend\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype_backend\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    676\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    677\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    678\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:265\u001B[39m, in \u001B[36mPyArrowImpl.read\u001B[39m\u001B[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001B[39m\n\u001B[32m    258\u001B[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001B[32m    259\u001B[39m     path,\n\u001B[32m    260\u001B[39m     filesystem,\n\u001B[32m    261\u001B[39m     storage_options=storage_options,\n\u001B[32m    262\u001B[39m     mode=\u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    263\u001B[39m )\n\u001B[32m    264\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m265\u001B[39m     pa_table = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mparquet\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpath_or_handle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    268\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    269\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    270\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    273\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m catch_warnings():\n\u001B[32m    274\u001B[39m         filterwarnings(\n\u001B[32m    275\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mignore\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    276\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mmake_block is deprecated\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    277\u001B[39m             \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[32m    278\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1824\u001B[39m, in \u001B[36mread_table\u001B[39m\u001B[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001B[39m\n\u001B[32m   1812\u001B[39m     \u001B[38;5;66;03m# TODO test that source is not a directory or a list\u001B[39;00m\n\u001B[32m   1813\u001B[39m     dataset = ParquetFile(\n\u001B[32m   1814\u001B[39m         source, read_dictionary=read_dictionary,\n\u001B[32m   1815\u001B[39m         memory_map=memory_map, buffer_size=buffer_size,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1821\u001B[39m         page_checksum_verification=page_checksum_verification,\n\u001B[32m   1822\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1824\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_threads\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1825\u001B[39m \u001B[43m                    \u001B[49m\u001B[43muse_pandas_metadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_pandas_metadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1475\u001B[39m, in \u001B[36mParquetDataset.read\u001B[39m\u001B[34m(self, columns, use_threads, use_pandas_metadata)\u001B[39m\n\u001B[32m   1467\u001B[39m         index_columns = [\n\u001B[32m   1468\u001B[39m             col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m _get_pandas_index_columns(metadata)\n\u001B[32m   1469\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(col, \u001B[38;5;28mdict\u001B[39m)\n\u001B[32m   1470\u001B[39m         ]\n\u001B[32m   1471\u001B[39m         columns = (\n\u001B[32m   1472\u001B[39m             \u001B[38;5;28mlist\u001B[39m(columns) + \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(index_columns) - \u001B[38;5;28mset\u001B[39m(columns))\n\u001B[32m   1473\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1475\u001B[39m table = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_table\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1476\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_filter_expression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1477\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_threads\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_threads\u001B[49m\n\u001B[32m   1478\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1480\u001B[39m \u001B[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001B[39;00m\n\u001B[32m   1481\u001B[39m \u001B[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001B[39;00m\n\u001B[32m   1482\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_pandas_metadata:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:589\u001B[39m, in \u001B[36mpyarrow._dataset.Dataset.to_table\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3941\u001B[39m, in \u001B[36mpyarrow._dataset.Scanner.to_table\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001B[39m, in \u001B[36mpyarrow.lib.pyarrow_internal_check_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\project\\javeriana\\repos\\open_corpus_co_es\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001B[39m, in \u001B[36mpyarrow.lib.check_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[31mOSError\u001B[39m: Couldn't deserialize thrift: TProtocolException: Invalid data\nDeserializing page header failed.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T23:54:08.459131Z",
     "start_time": "2025-06-23T23:54:06.907425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from open_corpus_co_es.loader import load_corpus, list_corpus\n",
    "from open_corpus_co_es.downloader import download_corpus\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "corpus_name = \"ods\"\n",
    "download_corpus(corpus_name, force=True)\n",
    "\n",
    "# Get the corpus directory\n",
    "corpus_dir = os.path.join(os.path.expanduser(\"~\"), \".open_corpus_co_es\", corpus_name)\n",
    "\n",
    "# Try to read files one by one\n",
    "data_frames = []\n",
    "for parquet_file in glob.glob(os.path.join(corpus_dir, \"**\", \"*.parquet\"), recursive=True):\n",
    "    try:\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        data_frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {parquet_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "if data_frames:\n",
    "    datos = pd.concat(data_frames, ignore_index=True)\n",
    "    print(type(datos))\n",
    "    print(datos)\n",
    "else:\n",
    "    print(\"No valid Parquet files were found or could be read\")"
   ],
   "id": "8dcc8cd7f8cf964",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Cargando corpus 'ods' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 285.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Corpus 'ods' listo en: C:\\Users\\phineas/.open_corpus_co_es/data\\ods\n",
      "No valid Parquet files were found or could be read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fd967cfd58897905"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
